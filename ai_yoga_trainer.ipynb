{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anmolkhurana490/Yoga-Posture-Recognition-and-Correction-System/blob/main/ai_yoga_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RgHOW2TjsoO",
        "outputId": "8354427b-994e-42de-9e2b-765dacaf5088",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Collecting numpy<2 (from mediapipe)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m461.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.15.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, numpy, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.21 numpy-1.26.4 protobuf-4.25.8 sounddevice-0.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4jHnoEYS43h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QddcE0wrjijF"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CchfuDaRg0xl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "def extract_zip(dir_name):\n",
        "  # Replace with the actual path to your zip file in Google Drive\n",
        "  zip_file_path = f'/content/drive/MyDrive/AI yoga trainer/large_dataset/{dir_name}.zip'\n",
        "  # Replace with the desired extraction location in Colab\n",
        "  extract_path = f'/content/large_dataset/{dir_name}'\n",
        "  try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(extract_path)\n",
        "    print(f\"Successfully unzipped {zip_file_path} to {extract_path}\")\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: Zip file not found at {zip_file_path}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "for dir_name in ['train', 'val', 'test']:\n",
        "  extract_zip(dir_name)"
      ],
      "metadata": {
        "id": "P7KNMvHwXcZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKXNbG7uTPW4"
      },
      "outputs": [],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa5aHL0Lje5L"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/large_dataset/train'\n",
        "validation_dir = '/content/large_dataset/val'\n",
        "test_dir = '/content/large_dataset/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dRN0cRmjhRg"
      },
      "outputs": [],
      "source": [
        "mpPose=mp.solutions.pose\n",
        "mpDraw=mp.solutions.drawing_utils\n",
        "pose=mpPose.Pose(min_detection_confidence=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqCfUXHTn7H6"
      },
      "outputs": [],
      "source": [
        "img=tf.io.read_file('/content/large_dataset/test/Akarna_Dhanurasana/Akarna_Dhanurasana_image_77.jpg')\n",
        "img=tf.image.decode_jpeg(img, channels=3)\n",
        "results=pose.process(img.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(img.numpy())\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "C_oiRXWZggC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rotate a point in 3D space by point (a,b,c)\n",
        "def rotate_point(point, angles, arbitrary_point):\n",
        "    \"\"\"\n",
        "    Rotate point (x,y,z) around pivot (a,b,c) by:\n",
        "      - alpha around X-axis\n",
        "      - beta  around Y-axis\n",
        "      - gamma around Z-axis\n",
        "    Angles in radians.\n",
        "    Returns (x', y', z').\n",
        "    \"\"\"\n",
        "\n",
        "    x, y, z = point\n",
        "    a, b, c = arbitrary_point\n",
        "    alpha, beta, gamma = angles\n",
        "\n",
        "    ca, sa = math.cos(alpha), math.sin(alpha)\n",
        "    cb, sb = math.cos(beta), math.sin(beta)\n",
        "    cg, sg = math.cos(gamma), math.sin(gamma)\n",
        "\n",
        "    x_prime = cg*(cb*(x - a) + sb*(sa*(y - b) + ca*(z - c))) - sg*(ca*(y - b) - sa*(z - c)) + a\n",
        "    y_prime = sg*(cb*(x - a) + sb*(sa*(y - b) + ca*(z - c))) + cg*(ca*(y - b) - sa*(z - c)) + b\n",
        "    z_prime = -sb*(x - a) + cb*(sa*(y - b) + ca*(z - c)) + c\n",
        "\n",
        "    return x_prime, y_prime, z_prime"
      ],
      "metadata": {
        "id": "R_5ktDzMFKen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_landmarks(image_path):\n",
        "  mp_drawing = mp.solutions.drawing_utils\n",
        "  mp_pose = mp.solutions.pose\n",
        "\n",
        "  with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = pose.process(image_rgb)\n",
        "\n",
        "    if results.pose_landmarks:\n",
        "      # Draw landmarks on the image\n",
        "      annotated_image = image.copy()\n",
        "      mp_drawing.draw_landmarks(\n",
        "          annotated_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
        "\n",
        "      # Display the annotated image using Matplotlib\n",
        "      plt.figure(figsize=(8, 8))\n",
        "      plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
        "      plt.title(\"Pose Landmarks\")\n",
        "      plt.axis('off')  # Hide axes\n",
        "      plt.show()\n",
        "    else:\n",
        "      print(\"No pose landmarks detected in the image.\")"
      ],
      "metadata": {
        "id": "mPgfY8xekDSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69WxCF4joHpF"
      },
      "outputs": [],
      "source": [
        "len(results.pose_landmarks.landmark)\n",
        "# print(results.pose_landmarks.landmark)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "annonated_image = img.numpy().copy()\n",
        "mpDraw.draw_landmarks(annonated_image, results.pose_landmarks, mpPose.POSE_CONNECTIONS)\n",
        "plt.imshow(annonated_image)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "KHKnS7jKij9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 7, figsize=(21, 3)) # 7 subplots in one row\n",
        "\n",
        "original_img_np = img.numpy().copy()\n",
        "axes[0].imshow(original_img_np)\n",
        "axes[0].scatter([landmark.x*len(img[0]) for landmark in results.pose_landmarks.landmark], [landmark.y*len(img) for landmark in results.pose_landmarks.landmark])\n",
        "axes[0].set_title(\"Original\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "angles_list = [1,2,3,4,5,6]\n",
        "\n",
        "for i, angle in enumerate(angles_list):\n",
        "  rotated_img_np = img.numpy().copy()\n",
        "  angles = (0, angle, 0) # Assuming rotation around Y-axis for visualization\n",
        "\n",
        "  rotated_landmarks = [rotate_point((landmark.x, landmark.y, landmark.z), angles, (0.5, 0.5, 0)) for landmark in results.pose_landmarks.landmark]\n",
        "\n",
        "  x_coords = [point[0] for point in rotated_landmarks]\n",
        "  y_coords = [point[1] for point in rotated_landmarks]\n",
        "  z_coords = [point[2] for point in rotated_landmarks]\n",
        "\n",
        "  connections = mpPose.POSE_CONNECTIONS\n",
        "  for connection in connections:\n",
        "    start_idx, end_idx = connection\n",
        "    axes[i+1].plot([x_coords[start_idx], x_coords[end_idx]],\n",
        "                [y_coords[start_idx], y_coords[end_idx]],\n",
        "                'blue', linewidth=2, alpha=0.7)\n",
        "\n",
        "  # Draw landmarks\n",
        "  axes[i+1].scatter(x_coords, y_coords, c='red', s=50, zorder=5)\n",
        "\n",
        "  # axes[i+1].imshow(rotated_img_np)\n",
        "  axes[i+1].invert_yaxis()\n",
        "  axes[i+1].set_title(f\"Rotation angle: {angle}\")\n",
        "  axes[i+1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FhxUOIlAFVWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbTBZtWxbS2s"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(os.listdir(validation_dir))\n",
        "num_classes = len(classes)\n",
        "print(\"Number of classes:\", num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes"
      ],
      "metadata": {
        "id": "-JVmaEFawqYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RK3CRDAduAz"
      },
      "outputs": [],
      "source": [
        "label_lookup = tf.keras.layers.StringLookup(\n",
        "    vocabulary = classes,\n",
        "    mask_token = None,\n",
        "    num_oov_indices = 0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def apply_noise_augmentation(keypoints, noise_factor=0.02):\n",
        "#     \"\"\"\n",
        "#     Add small random noise to keypoints\n",
        "#     \"\"\"\n",
        "#     noise = np.random.normal(0, noise_factor, keypoints.shape)\n",
        "#     return keypoints + noise\n",
        "\n",
        "def apply_scale_augmentation(keypoints, scale_range=(0.9, 1.1)):\n",
        "    \"\"\"\n",
        "    Apply random scaling to keypoints\n",
        "    \"\"\"\n",
        "    scale = random.uniform(scale_range[0], scale_range[1])\n",
        "    center = np.mean(keypoints, axis=0)\n",
        "    return center + (keypoints - center) * scale\n",
        "\n",
        "def comprehensive_augmentation(keypoints, num_augmentations=5):\n",
        "    \"\"\"\n",
        "    Apply comprehensive augmentation including rotation, noise, and scaling\n",
        "    \"\"\"\n",
        "    augmented_keypoints = []\n",
        "\n",
        "    # Original keypoints\n",
        "    augmented_keypoints.append(keypoints)\n",
        "\n",
        "    # # Calculate center point (hip center as pivot)\n",
        "    # left_hip = keypoints[23]  # Left hip landmark\n",
        "    # right_hip = keypoints[24]  # Right hip landmark\n",
        "    # center_point = (left_hip + right_hip) / 2\n",
        "    center_point = np.mean(keypoints, axis=0)\n",
        "\n",
        "    for _ in range(num_augmentations):\n",
        "        # Apply Rotation Augmentation\n",
        "        # Angles are in radians.\n",
        "        alpha = random.uniform(-0.5, 0.5) # approx -28 to 28 degrees\n",
        "        beta = random.uniform(-1.5, 1.5) # approx -86 to 86 degrees\n",
        "        gamma = random.uniform(-0.3, 0.3) # approx -17 to 17 degrees\n",
        "        angles = (alpha, beta, gamma)\n",
        "\n",
        "        rotated_keypoints = [rotate_point(point, angles, center_point) for point in keypoints]\n",
        "        current_keypoints = np.array(rotated_keypoints)\n",
        "\n",
        "        # Apply noise\n",
        "        # current_keypoints = apply_noise_augmentation(current_keypoints, noise_factor=0.015)\n",
        "\n",
        "        # Apply scaling\n",
        "        current_keypoints = apply_scale_augmentation(current_keypoints, scale_range=(0.95, 1.05))\n",
        "\n",
        "        augmented_keypoints.append(current_keypoints)\n",
        "\n",
        "    return augmented_keypoints"
      ],
      "metadata": {
        "id": "WDdWyDOeJTPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kmajrNfk9rr"
      },
      "outputs": [],
      "source": [
        "tf.data.experimental.enable_debug_mode()\n",
        "def process_image_with_augmentation(image_path, num_augment):\n",
        "  label = tf.strings.split(image_path, os.sep)[-2]\n",
        "  encoded_label = label_lookup(label)\n",
        "\n",
        "  img = tf.io.read_file(image_path)\n",
        "\n",
        "  try:\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "  except:\n",
        "    return np.zeros((0,3)), encoded_label\n",
        "\n",
        "  img = tf.image.resize(img, [128, 128])\n",
        "\n",
        "  def extract_features(img):\n",
        "    features_list = []\n",
        "    img = tf.cast(img, dtype=tf.uint8).numpy()\n",
        "\n",
        "    results = pose.process(img)\n",
        "    if results.pose_landmarks:\n",
        "      features = []\n",
        "      for landmark in results.pose_landmarks.landmark:\n",
        "        features.append([landmark.x, landmark.y, landmark.z])\n",
        "      features = np.array(features)\n",
        "\n",
        "      # Apply comprehensive augmentation\n",
        "      augmented_keypoints_list = comprehensive_augmentation(\n",
        "        features, num_augmentations=num_augment\n",
        "      )\n",
        "\n",
        "      # Convert to the format expected by the model\n",
        "      for keypoints in augmented_keypoints_list:\n",
        "        features_list.append(keypoints)\n",
        "\n",
        "      return np.array(features_list)\n",
        "    else:\n",
        "      return np.zeros((0,3))\n",
        "\n",
        "  features = tf.py_function(extract_features, [img], [tf.float32])\n",
        "  return features, encoded_label\n",
        "\n",
        "def split_points(features, labels):\n",
        "  features = tf.reshape(features, [-1, 33, 3])\n",
        "  labels = tf.repeat(labels, tf.shape(features)[0])\n",
        "  return tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "\n",
        "def extract_keypoints_dataset_with_augmentation(path, num_augment=0):\n",
        "  dataset = tf.data.Dataset.list_files(f'{path}/*/*', shuffle=False)\n",
        "\n",
        "  processed_dataset = dataset.map(\n",
        "    lambda x: process_image_with_augmentation(x, num_augment),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        "  )\n",
        "  processed_dataset = processed_dataset.filter(lambda x,y: tf.shape(x)[1]>0)  # Remove out None values\n",
        "  processed_dataset = processed_dataset.flat_map(split_points)  # Flatten the dataset\n",
        "  return processed_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbBVR06T3e2g"
      },
      "outputs": [],
      "source": [
        "train_dataset = extract_keypoints_dataset_with_augmentation(train_dir, 10).shuffle(1000).cache()\n",
        "for data in train_dataset.take(5):\n",
        "  print(\"Features data shape:\", data[0].numpy().shape, \"Label:\", data[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuRW_xaIpagJ"
      },
      "outputs": [],
      "source": [
        "validation_dataset = extract_keypoints_dataset_with_augmentation(validation_dir).cache()\n",
        "for data in validation_dataset.take(5):\n",
        "  print(\"Features data shape:\", data[0].numpy().shape, \"Label:\", data[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q5KvVLZp6_p"
      },
      "outputs": [],
      "source": [
        "test_dataset = extract_keypoints_dataset_with_augmentation(test_dir).cache()\n",
        "for data in test_dataset.take(5):\n",
        "  print(\"Features data shape:\", data[0].numpy().shape, \"Label:\", data[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8Zx1416nvnG"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.batch(256).prefetch(tf.data.AUTOTUNE).repeat()\n",
        "validation_dataset = validation_dataset.batch(256).prefetch(tf.data.AUTOTUNE).repeat()\n",
        "test_dataset = test_dataset.batch(256).prefetch(tf.data.AUTOTUNE).repeat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc5By434TZa9"
      },
      "outputs": [],
      "source": [
        "# # Image data generators\n",
        "# train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
        "#                                    rotation_range=20,\n",
        "#                                    width_shift_range=0.2,\n",
        "#                                    height_shift_range=0.2,\n",
        "#                                    shear_range=0.2,\n",
        "#                                    zoom_range=0.2,\n",
        "#                                    horizontal_flip=True,\n",
        "#                                    fill_mode='nearest')\n",
        "\n",
        "# validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "# test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "# train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "#                                                     target_size=(150, 150),\n",
        "#                                                     batch_size=32,\n",
        "#                                                     class_mode='categorical')\n",
        "\n",
        "# validation_generator = validation_datagen.flow_from_directory(validation_dir,\n",
        "#                                                               target_size=(150, 150),\n",
        "#                                                               batch_size=32,\n",
        "#                                                               class_mode='categorical')\n",
        "\n",
        "# test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "#                                                   target_size=(150, 150),\n",
        "#                                                   batch_size=32,\n",
        "#                                                   class_mode='categorical',\n",
        "#                                                   shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk7ddP0ZnHTA"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(33, 3)),\n",
        "    Flatten(),\n",
        "    BatchNormalization(),\n",
        "    Dense(1024, activation='leaky_relu'),\n",
        "    Dropout(0.3),\n",
        "    BatchNormalization(),\n",
        "    Dense(768, activation='leaky_relu'),\n",
        "    Dropout(0.3),\n",
        "    BatchNormalization(),\n",
        "    Dense(512, activation='leaky_relu'),\n",
        "    Dropout(0.3),\n",
        "    BatchNormalization(),\n",
        "    Dense(256, activation='leaky_relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca6-SFxWaKkF"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=300,\n",
        "    steps_per_epoch=450,\n",
        "    validation_data=validation_dataset,\n",
        "    validation_steps=65\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrrbdLvjgw2G"
      },
      "outputs": [],
      "source": [
        "# test_loss, test_acc = model.evaluate(test_dataset)\n",
        "# print(f\"Test accuracy: {test_acc}\")\n",
        "model.save('yoga_trainer_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t86em2WIm1ez"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkWghKtimwig"
      },
      "outputs": [],
      "source": [
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifbP2By68zAR"
      },
      "outputs": [],
      "source": [
        "img=tf.io.read_file('/content/large_dataset/test/Akarna_Dhanurasana/Akarna_Dhanurasana_image_76.jpg')\n",
        "img=tf.image.decode_jpeg(img, channels=3)\n",
        "results=pose.process(img.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features=[[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]\n",
        "ypred=model.predict(np.array([features])).argmax(axis=1)\n",
        "print('Pose Predicted:', label_lookup.get_vocabulary()[ypred[0]])"
      ],
      "metadata": {
        "id": "GefUbp8sdJl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_lookup.save_assets('label_lookup_')"
      ],
      "metadata": {
        "id": "8ARxoHRceUnu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}